twitclean <- tm_map(twitclean, removetitik2)
removetitikkoma <- function(y) gsub(";", " ", y)
twitclean <- tm_map(twitclean, removetitikkoma)
removetitik3 <- function(y) gsub("p.", "", y)
twitclean <- tm_map(twitclean, removetitik3)
removeamp <- function(y) gsub("&amp;", "", y)
twitclean <- tm_map(twitclean, removeamp)
removeUN <- function(z) gsub("@\\w+", "", z)
twitclean <- tm_map(twitclean, removeUN)
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
twitclean <- tm_map(twitclean,remove.all)
#Menghapus  titik koma, menjadi non kapital
twitclean <- tm_map(twitclean, removePunctuation)
twitclean <- tm_map(twitclean, tolower)
twitclean <- tm_map(twitclean , removeWords,
c('corona','virus','jokowi','joko',
'widodo','covid19','covid-19','coronavirus' , 'viruscorona' , 'indonesian' , 'government' , 'outbreak' , 'cant' , 'taking' , 'bcs' , 'covid', 'indian','embas','governments' , 'will' , 'care' , 'adds' , 'rsup' , 'new', 'rsal','indonesia','like','dont','jakarta','international','just' ,'the', 'and', 'this', 'for', 'this', 'that', 'has', 'about', 'with','are','not','from','its','about'))
install.packages("xlsx")
replacecomma <- function(y) gsub(",", "", y)
library(twitteR)
library(RCurl)
library(wordcloud)
library(corpus)
library(tm)
library(xlsx)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('indonesian government -filter:retweets', n = 12000, lang = "en",retryOnRateLimit = 10e3)
##save dulu datanya
saveRDS(some_tweets,file = 'tweetfovfilter2.rds')
##load datanya
tw <- readRDS('tweetfovfilter2.rds')
d2 = twListToDF(tw)
write.xlsx(d2, "twittergov2.xlsx")
remove(tw)
#### hapus data
komen2 <- d2$text
textclean <- Corpus(VectorSource(komen2))
#### cleaning data
##Cleaning data
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
twitclean <- tm_map(textclean, removeURL)
removeNL <- function(y) gsub("\n", " ", y)
twitclean <- tm_map(twitclean, removeNL)
replacecomma <- function(y) gsub(",", "", y)
twitclean <- tm_map(twitclean, replacecomma)
removeRT <- function(y) gsub("RT ", "", y)
twitclean <- tm_map(twitclean, removeRT)
removetitik2 <- function(y) gsub(":", "", y)
twitclean <- tm_map(twitclean, removetitik2)
removetitikkoma <- function(y) gsub(";", " ", y)
twitclean <- tm_map(twitclean, removetitikkoma)
removetitik3 <- function(y) gsub("p.", "", y)
twitclean <- tm_map(twitclean, removetitik3)
removeamp <- function(y) gsub("&amp;", "", y)
twitclean <- tm_map(twitclean, removeamp)
removeUN <- function(z) gsub("@\\w+", "", z)
twitclean <- tm_map(twitclean, removeUN)
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
twitclean <- tm_map(twitclean,remove.all)
#Menghapus  titik koma, menjadi non kapital
twitclean <- tm_map(twitclean, removePunctuation)
twitclean <- tm_map(twitclean, tolower)
twitclean <- tm_map(twitclean , removeWords,
c('corona','virus','jokowi','joko',
'widodo','covid19','covid-19','coronavirus' , 'viruscorona' , 'indonesian' , 'government' , 'outbreak' , 'cant' , 'taking' , 'bcs' , 'covid', 'indian','embas','governments' , 'will' , 'care' , 'adds' , 'rsup' , 'new', 'rsal','indonesia','like','dont','jakarta','international','just' ,'the', 'and', 'this', 'for', 'this', 'that', 'has', 'about', 'with','are','not','from','its','about'))
library(twitteR)
library(RCurl)
library(wordcloud)
library(corpus)
library(tm)
library(xlsx)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('indonesian government -filter:retweets', n = 12000, lang = "en",retryOnRateLimit = 10e3)
##save dulu datanya
saveRDS(some_tweets,file = 'tweetfovfilter2.rds')
##load datanya
tw <- readRDS('tweetfovfilter2.rds')
d2 = twListToDF(tw)
write.xlsx(d2, "twittergov2.xlsx")
remove(tw)
#### hapus data
komen2 <- d2$text
textclean <- Corpus(VectorSource(komen2))
#### cleaning data
##Cleaning data
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
twitclean <- tm_map(textclean, removeURL)
removeNL <- function(y) gsub("\n", " ", y)
twitclean <- tm_map(twitclean, removeNL)
replacecomma <- function(y) gsub(",", "", y)
twitclean <- tm_map(twitclean, replacecomma)
removeRT <- function(y) gsub("RT ", "", y)
twitclean <- tm_map(twitclean, removeRT)
removetitik2 <- function(y) gsub(":", "", y)
twitclean <- tm_map(twitclean, removetitik2)
removetitikkoma <- function(y) gsub(";", " ", y)
twitclean <- tm_map(twitclean, removetitikkoma)
removetitik3 <- function(y) gsub("p.", "", y)
twitclean <- tm_map(twitclean, removetitik3)
removeamp <- function(y) gsub("&amp;", "", y)
twitclean <- tm_map(twitclean, removeamp)
removeUN <- function(z) gsub("@\\w+", "", z)
twitclean <- tm_map(twitclean, removeUN)
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
twitclean <- tm_map(twitclean,remove.all)
#Menghapus  titik koma, menjadi non kapital
twitclean <- tm_map(twitclean, removePunctuation)
twitclean <- tm_map(twitclean, tolower)
twitclean <- tm_map(twitclean , removeWords,
c('corona','virus','jokowi','joko',
'widodo','covid19','covid-19','coronavirus' , 'viruscorona' , 'indonesian' , 'government' , 'outbreak' , 'cant' , 'taking' , 'bcs' , 'covid', 'indian','embas','governments' , 'will' , 'care' , 'adds' , 'rsup' , 'new', 'rsal','indonesia','like','dont','jakarta','international','just' ,'the', 'and', 'this', 'for', 'this', 'that', 'has', 'about', 'with','are','not','from','its','about'))
stop = stopwords(kind = "en")
twitclean <- tm_map(twitclean , removeWords, stop)
twitclean <- tm_map(twitclean, content_transformer(tolower))
twitclean
install.packages("twitteR")
install.packages("stringr")
install.packages("xlsx")
install.packages("plyr")
install.packages("stringr")
install.packages("wordcloud")
install.packages("plyr")
library(twitteR)
library(wordcloud)
library(tm)
library(xlsx)
library(stringr)
library(plyr)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('indonesian government -filter:retweets', n = 12000, lang = "en",retryOnRateLimit = 10e3)
searchTwitter("MRT", n=500, lang="id")
tweets = searchTwitter("MRT", n=500, lang="id")
# convert tweets ke data frame
tweets_df = twListToDF(tweets)
# extract text content di tweet pertama
tweets[[1]]$getText()
# extract user name di tweet pertama
tweets[[1]]$getScreenName()
# extract tweet Id di tweet pertama
tweets[[1]]$getId()
# extract tanggal dan waktu posting di tweet pertama
tweets[[1]]$getCreated()
# untuk mengetahui di tweet menggunakan perangkat apa
tweets[[1]]$getStatusSource()
# extract text content di semua tweet
sapply(tweets, function(x) x$getText())
# extract  user name di semua tweet
sapply(tweets, function(x) x$getScreenName())
# extract Id number di semua tweet
sapply(tweets, function(x) x$getId())
# extract tanggal dan waktu posting di semua tweet
sapply(tweets, function(x) x$getCreated())
# untuk mengetahui di tweet menggunakan perangkat apa
sapply(tweets, function(x) x$getStatusSource())
# Extract the text from the tweets in a vector
tweets_text = sapply(tweets, function(x) x$getText())
#Buat Korpus
tweets_corpus= Corpus(VectorSource(tweets_text))
#Buat document term matriks untuk menerapkan beberapa perubahan dengan stopwords
tdm = TermDocumentMatrix(tweets_corpus, control = list(removePunctuation = TRUE, stopwords = c("MRT", "jakarta", stopwords("id", source="stopwords-iso")), removeNumbers = TRUE, tolower = TRUE))
# definisi tdm sebagai matriks
m = as.matrix(tdm)
# Frekuensi kemunculan kata
word_freqs = sort(rowSums(m), decreasing=TRUE)
# Membuat data frame dengan kata-kata dan frekuensinya
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# simpan image ke format png
png("MRTCloud.png", width=12, height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
library(twitteR)
library(wordcloud)
library(tm)
library(xlsx)
library(stringr)
library(plyr)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('indonesian government -filter:retweets', n = 12000, lang = "en",retryOnRateLimit = 10e3)
searchTwitter("MRT", n=500, lang="id")
tweets = searchTwitter("MRT", n=500, lang="id")
# convert tweets ke data frame
tweets_df = twListToDF(tweets)
# extract text content di tweet pertama
tweets[[1]]$getText()
# extract user name di tweet pertama
tweets[[1]]$getScreenName()
# extract tweet Id di tweet pertama
tweets[[1]]$getId()
# extract tanggal dan waktu posting di tweet pertama
tweets[[1]]$getCreated()
# untuk mengetahui di tweet menggunakan perangkat apa
tweets[[1]]$getStatusSource()
# extract text content di semua tweet
sapply(tweets, function(x) x$getText())
# extract  user name di semua tweet
sapply(tweets, function(x) x$getScreenName())
# extract Id number di semua tweet
sapply(tweets, function(x) x$getId())
# extract tanggal dan waktu posting di semua tweet
sapply(tweets, function(x) x$getCreated())
# untuk mengetahui di tweet menggunakan perangkat apa
sapply(tweets, function(x) x$getStatusSource())
# Extract the text from the tweets in a vector
tweets_text = sapply(tweets, function(x) x$getText())
#Buat Korpus
tweets_corpus= Corpus(VectorSource(tweets_text))
#Buat document term matriks untuk menerapkan beberapa perubahan dengan stopwords
tdm = TermDocumentMatrix(tweets_corpus, control = list(removePunctuation = TRUE, stopwords = c("MRT", "jakarta", stopwords("id")), removeNumbers = TRUE, tolower = TRUE))
# definisi tdm sebagai matriks
m = as.matrix(tdm)
# Frekuensi kemunculan kata
word_freqs = sort(rowSums(m), decreasing=TRUE)
# Membuat data frame dengan kata-kata dan frekuensinya
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# simpan image ke format png
png("MRTCloud.png", width=12, height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
install.packages("stopwords")
library(twitteR)
library(wordcloud)
library(tm)
library(xlsx)
library(stringr)
library(plyr)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('indonesian government -filter:retweets', n = 12000, lang = "en",retryOnRateLimit = 10e3)
searchTwitter("MRT", n=500, lang="id")
tweets = searchTwitter("MRT", n=500, lang="id")
# convert tweets ke data frame
tweets_df = twListToDF(tweets)
# extract text content di tweet pertama
tweets[[1]]$getText()
# extract user name di tweet pertama
tweets[[1]]$getScreenName()
# extract tweet Id di tweet pertama
tweets[[1]]$getId()
# extract tanggal dan waktu posting di tweet pertama
tweets[[1]]$getCreated()
# untuk mengetahui di tweet menggunakan perangkat apa
tweets[[1]]$getStatusSource()
# extract text content di semua tweet
sapply(tweets, function(x) x$getText())
# extract  user name di semua tweet
sapply(tweets, function(x) x$getScreenName())
# extract Id number di semua tweet
sapply(tweets, function(x) x$getId())
# extract tanggal dan waktu posting di semua tweet
sapply(tweets, function(x) x$getCreated())
# untuk mengetahui di tweet menggunakan perangkat apa
sapply(tweets, function(x) x$getStatusSource())
# Extract the text from the tweets in a vector
tweets_text = sapply(tweets, function(x) x$getText())
#Buat Korpus
tweets_corpus= Corpus(VectorSource(tweets_text))
#Buat document term matriks untuk menerapkan beberapa perubahan dengan stopwords
tdm = TermDocumentMatrix(tweets_corpus, control = list(removePunctuation = TRUE, stopwords = c("MRT", "jakarta", stopwords("id", source="stopwords-iso")), removeNumbers = TRUE, tolower = TRUE))
# definisi tdm sebagai matriks
m = as.matrix(tdm)
# Frekuensi kemunculan kata
word_freqs = sort(rowSums(m), decreasing=TRUE)
# Membuat data frame dengan kata-kata dan frekuensinya
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# simpan image ke format png
png("MRTCloud.png", width=12, height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
install.packages("stopwords")
library(twitteR)
library(wordcloud)
library(tm)
library(xlsx)
library(stringr)
library(plyr)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('indonesian government -filter:retweets', n = 12000, lang = "en",retryOnRateLimit = 10e3)
searchTwitter("MRT", n=500, lang="id")
tweets = searchTwitter("MRT", n=500, lang="id")
# convert tweets ke data frame
tweets_df = twListToDF(tweets)
# extract text content di tweet pertama
tweets[[1]]$getText()
# extract user name di tweet pertama
tweets[[1]]$getScreenName()
# extract tweet Id di tweet pertama
tweets[[1]]$getId()
# extract tanggal dan waktu posting di tweet pertama
tweets[[1]]$getCreated()
# untuk mengetahui di tweet menggunakan perangkat apa
tweets[[1]]$getStatusSource()
# extract text content di semua tweet
sapply(tweets, function(x) x$getText())
# extract  user name di semua tweet
sapply(tweets, function(x) x$getScreenName())
# extract Id number di semua tweet
sapply(tweets, function(x) x$getId())
# extract tanggal dan waktu posting di semua tweet
sapply(tweets, function(x) x$getCreated())
# untuk mengetahui di tweet menggunakan perangkat apa
sapply(tweets, function(x) x$getStatusSource())
# Extract the text from the tweets in a vector
tweets_text = sapply(tweets, function(x) x$getText())
#Buat Korpus
tweets_corpus= Corpus(VectorSource(tweets_text))
#Buat document term matriks untuk menerapkan beberapa perubahan dengan stopwords
tdm = TermDocumentMatrix(tweets_corpus, control = list(removePunctuation = TRUE, stopwords = c("MRT", "jakarta")), removeNumbers = TRUE, tolower = TRUE))
# definisi tdm sebagai matriks
m = as.matrix(tdm)
# Frekuensi kemunculan kata
word_freqs = sort(rowSums(m), decreasing=TRUE)
# Membuat data frame dengan kata-kata dan frekuensinya
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# simpan image ke format png
png("MRTCloud.png", width=12, height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
install.packages("stopwords")
library(twitteR)
library(wordcloud)
library(tm)
library(xlsx)
library(stringr)
library(plyr)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('indonesian government -filter:retweets', n = 12000, lang = "en",retryOnRateLimit = 10e3)
searchTwitter("MRT", n=500, lang="id")
tweets = searchTwitter("MRT", n=500, lang="id")
# convert tweets ke data frame
tweets_df = twListToDF(tweets)
# extract text content di tweet pertama
tweets[[1]]$getText()
# extract user name di tweet pertama
tweets[[1]]$getScreenName()
# extract tweet Id di tweet pertama
tweets[[1]]$getId()
# extract tanggal dan waktu posting di tweet pertama
tweets[[1]]$getCreated()
# untuk mengetahui di tweet menggunakan perangkat apa
tweets[[1]]$getStatusSource()
# extract text content di semua tweet
sapply(tweets, function(x) x$getText())
# extract  user name di semua tweet
sapply(tweets, function(x) x$getScreenName())
# extract Id number di semua tweet
sapply(tweets, function(x) x$getId())
# extract tanggal dan waktu posting di semua tweet
sapply(tweets, function(x) x$getCreated())
# untuk mengetahui di tweet menggunakan perangkat apa
sapply(tweets, function(x) x$getStatusSource())
# Extract the text from the tweets in a vector
tweets_text = sapply(tweets, function(x) x$getText())
#Buat Korpus
tweets_corpus= Corpus(VectorSource(tweets_text))
#Buat document term matriks untuk menerapkan beberapa perubahan dengan stopwords
tdm = TermDocumentMatrix(tweets_corpus, control = list(removePunctuation = TRUE, stopwords = TRUE, removeNumbers = TRUE, tolower = TRUE))
# definisi tdm sebagai matriks
m = as.matrix(tdm)
# Frekuensi kemunculan kata
word_freqs = sort(rowSums(m), decreasing=TRUE)
# Membuat data frame dengan kata-kata dan frekuensinya
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# simpan image ke format png
png("MRTCloud.png", width=12, height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
install.packages("stopwords")
library(twitteR)
library(wordcloud)
library(tm)
library(xlsx)
library(stringr)
library(plyr)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('covid+haji', n = 12000, lang = "en",retryOnRateLimit = 10e3)
searchTwitter("MRT", n=500, lang="id")
tweets = searchTwitter("MRT", n=500, lang="id")
# convert tweets ke data frame
tweets_df = twListToDF(tweets)
# extract text content di tweet pertama
tweets[[1]]$getText()
# extract user name di tweet pertama
tweets[[1]]$getScreenName()
# extract tweet Id di tweet pertama
tweets[[1]]$getId()
# extract tanggal dan waktu posting di tweet pertama
tweets[[1]]$getCreated()
# untuk mengetahui di tweet menggunakan perangkat apa
tweets[[1]]$getStatusSource()
# extract text content di semua tweet
sapply(tweets, function(x) x$getText())
# extract  user name di semua tweet
sapply(tweets, function(x) x$getScreenName())
# extract Id number di semua tweet
sapply(tweets, function(x) x$getId())
# extract tanggal dan waktu posting di semua tweet
sapply(tweets, function(x) x$getCreated())
# untuk mengetahui di tweet menggunakan perangkat apa
sapply(tweets, function(x) x$getStatusSource())
# Extract the text from the tweets in a vector
tweets_text = sapply(tweets, function(x) x$getText())
#Buat Korpus
tweets_corpus= Corpus(VectorSource(tweets_text))
#Buat document term matriks untuk menerapkan beberapa perubahan dengan stopwords
tdm = TermDocumentMatrix(tweets_corpus, control = list(removePunctuation = TRUE, stopwords = TRUE, removeNumbers = TRUE, tolower = TRUE))
# definisi tdm sebagai matriks
m = as.matrix(tdm)
# Frekuensi kemunculan kata
word_freqs = sort(rowSums(m), decreasing=TRUE)
# Membuat data frame dengan kata-kata dan frekuensinya
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# simpan image ke format png
png("MRTCloud.png", width=12, height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
library(twitteR)
library(wordcloud)
library(tm)
library(xlsx)
library(stringr)
library(plyr)
consumer_key = "m2BwVwnoJV99e1KwemZZLOqGk"
consumer_secret = "eBluHf419cPTCD3WxgTd6YnuVFCOL80DB2Hb5J5brLRsPuwomd"
access_token = "69820722-g3nAAdyhgnQEPu8Vipq1jWY7dmVD50L4wITNK52G9"
access_secret ="eK0dPekfF1GHSoLOWJ6PyCsGyUfZs9gcnDU13xsMYPA6E"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
some_tweets2 = searchTwitter('covid+haji', n = 12000, lang = "en",retryOnRateLimit = 10e3)
searchTwitter("covid+haji", n=500, lang="id")
tweets = searchTwitter("covid+haji", n=500, lang="id")
# convert tweets ke data frame
tweets_df = twListToDF(tweets)
# extract text content di tweet pertama
tweets[[1]]$getText()
# extract user name di tweet pertama
tweets[[1]]$getScreenName()
# extract tweet Id di tweet pertama
tweets[[1]]$getId()
# extract tanggal dan waktu posting di tweet pertama
tweets[[1]]$getCreated()
# untuk mengetahui di tweet menggunakan perangkat apa
tweets[[1]]$getStatusSource()
# extract text content di semua tweet
sapply(tweets, function(x) x$getText())
# extract  user name di semua tweet
sapply(tweets, function(x) x$getScreenName())
# extract Id number di semua tweet
sapply(tweets, function(x) x$getId())
# extract tanggal dan waktu posting di semua tweet
sapply(tweets, function(x) x$getCreated())
# untuk mengetahui di tweet menggunakan perangkat apa
sapply(tweets, function(x) x$getStatusSource())
# Extract the text from the tweets in a vector
tweets_text = sapply(tweets, function(x) x$getText())
#Buat Korpus
tweets_corpus= Corpus(VectorSource(tweets_text))
#Buat document term matriks untuk menerapkan beberapa perubahan dengan stopwords
tdm = TermDocumentMatrix(tweets_corpus, control = list(removePunctuation = TRUE, stopwords = TRUE, removeNumbers = TRUE, tolower = TRUE))
# definisi tdm sebagai matriks
m = as.matrix(tdm)
# Frekuensi kemunculan kata
word_freqs = sort(rowSums(m), decreasing=TRUE)
# Membuat data frame dengan kata-kata dan frekuensinya
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# simpan image ke format png
png("hjcovidCloud.png", width=12, height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
